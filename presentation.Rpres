SVM Classification Sample
========================================================
author: Can Candan
date: `r date()`
css: custom.css


The Data
========================================================

This is a shiny adaptation of the lab from page 363 of Introduction to Statistical Learning book.
The data consists of 200 (x,y) points with 100 of those with x centered around 2, 50 around -2 and 50 around 0. The first 150 has been assigned a to the class 1 and the rest to 2.  

```{r}
library(e1071)
set.seed (1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))

```

Fitting the SVM
========================================================
We are using half of the data as training set 
```{r}
train=sample(200,100)
```

And fitting the svm with the values obtained from the sliders
```
svmfit=svm(y~., data=dat[train,], kernel="radial", gamma=input$gamma,cost=10^input$cost)
```


Best fit
========================================================
type: kucuk
Using tune function from svm library we get the best cross validation parameters 


```{r,width=800}
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4) ))
summary(tune.out)$best.parameters
```


Bias Variance Tradeoff 1
========================================================
Note that by increasing the cost we can reduce the number of training errors, here cost=1e6, gamma=2
```{r,echo=FALSE}
set.seed(1)
fit=svm(y~., data=dat[train,], kernel="radial", gamma=2,cost=1e6)
table(true=dat[train,"y"], pred=predict(fit, newx=dat[train ,]))
```

However it performs poorly on the test data due to overfitting
```{r,echo=FALSE}
set.seed(1)
fit=svm(y~., data=dat[train,], kernel="radial", gamma=2,cost=1e6)
table(true=dat[-train,"y"], pred=predict(fit, newx=dat[-train ,]))
```




Bias Variance Tradeoff 2
========================================================
Note that by increasing the gamma the fit becomes more nonlinear and fitting the training set better, here gamma=10, cost=1
```{r,echo=FALSE}
set.seed(1)
fit=svm(y~., data=dat[train,], kernel="radial", gamma=100,cost=1)
table(true=dat[train,"y"], pred=predict(fit, newx=dat[train ,]))
```

However we see the tradeoff again
```{r,echo=FALSE}
set.seed(1)
fit=svm(y~., data=dat[train,], kernel="radial", gamma=100,cost=1)
table(true=dat[-train,"y"], pred=predict(fit, newx=dat[-train ,]))
```

